# ğŸ“š RAG PDF Assistant with Streamlit

An interactive **Retrieval-Augmented Generation (RAG)** application built with **Streamlit**. It lets you upload PDF or text files, indexes their content into **Qdrant** using **LangChain**, and then allows you to query them using **OpenAI GPT models**.

This app is perfect for quickly building a knowledge base from your documents and asking natural language questions to retrieve answers with context.

---

## âœ¨ Features

âœ… Upload PDF or TXT files  
âœ… Automatic chunking and indexing with LangChain  
âœ… Store and retrieve vector embeddings using Qdrant  
âœ… Query your documents with GPT-4 (or GPT-3.5) using OpenAI API  
âœ… Simple and intuitive Streamlit interface

---

## ğŸ“¦ Tech Stack

- [Streamlit](https://streamlit.io/) â€“ For building the web UI
- [LangChain](https://www.langchain.com/) â€“ Document loading, splitting, and retrieval
- [Qdrant](https://qdrant.tech/) â€“ Vector database for storing embeddings
- [OpenAI](https://platform.openai.com/docs) â€“ GPT models for answering questions
- [dotenv](https://pypi.org/project/python-dotenv/) â€“ Manage environment variables

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/rag-pdf-assistant.git

cd streamlit_rag
```

### 2ï¸âƒ£ Set Up Python Environment

We recommend using a virtual environment:

```bash
virtualenv .venv

source venv/bin/activate # On Windows use venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Environment Variables

Create a .env file in the root directory with your OpenAI API key:

```bash
OPENAI_API_KEY=your-openai-api-key
```

---

## ğŸ³ Running Qdrant with Docker

This app requires **Qdrant** as the vector database. You can run it in two ways:

---

### Option 1: Run Qdrant directly with Docker (if installed locally)

```bash
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

### Option 2: Use Docker Compose (Recommended)

We provide a docker-compose.yml file. To pull the Qdrant image and run it:

```bash
docker compose up -d
```

This will:

- âœ… Download the Qdrant image
- âœ… Start it as a background service

To stop it later:

```bash
docker compose down
```

---

## ğŸ“ How It Works

### ğŸ“¤ Step 1: Upload and Index

- Upload a PDF or TXT file.
- The app uses **LangChain** to:
  - Load your document
  - Split it into chunks
  - Generate vector embeddings with OpenAIâ€™s `text-embedding-3-large` model
  - Store them in **Qdrant** (running locally or remotely)

### ğŸ” Step 2: Query Your Knowledge Base

- Ask any question about the uploaded document.
- The app:
  - Searches for relevant chunks in Qdrant
  - Provides context to GPT-4
  - Returns a concise answer with references to page numbers.

---

## ğŸ–¥ï¸ Running the App

Make sure **Qdrant** is running locally at `http://localhost:6333` (or change the URL in the code if using a remote instance). You can start Qdrant using Docker:

```bash
docker run -p 6333:6333 qdrant/qdrant
```

Then, run the Streamlit app:

```bash
streamlit run streamlit.py
```

The app will open in your browser at http://localhost:8501.
